{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to CloseAi","text":""},{"location":"#introduction","title":"Introduction","text":"<ul> <li>This is the documentation for <code>CloseAi</code> our question and answer system that uses OpenAI embeddings and Supabase vector database.</li> <li>The document serves as a guide to understanding the system and how to use it effectively.</li> <li><code>mkdocs build</code> - Build the documentation site.</li> <li><code>mkdocs -h</code> - Print help message and exit.</li> </ul>"},{"location":"#why-us","title":"Why us?","text":"<ul> <li>Our system provides accurate and relevant answers to user queries.   It uses OpenAI embeddings to analyze the meaning of user queries and match them with the most relevant answers.</li> <li>It also utilizes a Supabase vector database for fast and efficient storage and retrieval of data.</li> <li>The system offers a powerful and user-friendly tool for providing accurate and relevant answers to user queries.</li> <li>This documentation will provide all the necessary information for using the system effectively.</li> </ul>"},{"location":"#what-is-embeddings","title":"What is embeddings?","text":"<ul> <li>In natural language processing, words are represented as numerical vectors called embeddings.</li> <li>Embeddings are designed to capture the meaning of words and the relationships between them.</li> <li>For example, the word \"cat\" might be represented as a vector that includes information about its appearance, behavior, and typical environment.</li> <li>Similarly, the word \"dog\" might be represented as a vector that includes information about its appearance, behavior, and typical environment.</li> <li>By comparing the embeddings of different words, it's possible to identify relationships between them. For instance, the embeddings for \"cat\" and \"dog\" might be similar, indicating that these words are related to each other in some way.</li> <li>Embeddings can be generated using various techniques, such as neural networks or dimensionality reduction algorithms.</li> <li>In natural language processing applications, embeddings are often used to improve the accuracy of tasks like text classification, sentiment analysis, and language translation.</li> </ul>"},{"location":"#what-is-supabase-vector-database","title":"What is Supabase vector database?","text":"<ul> <li>It is built on top of PostgreSQL, a popular open-source relational database management system.</li> <li>It includes specialized indexing and query functionality that allows for efficient storage and retrieval of vector data.</li> <li>Supabase vector database is designed to work seamlessly with machine learning frameworks like TensorFlow and PyTorch, as well as with popular NLP libraries like spaCy and Hugging Face Transformers.</li> <li>It is highly scalable and can handle large volumes of data, making it ideal for use in applications that involve processing large amounts of text or image data.</li> <li>Supabase vector database is open-source and free to use, making it accessible to developers and organizations of all sizes.</li> <li>It includes a user-friendly interface and a variety of tools and resources to help developers get started with using the database.</li> </ul>"},{"location":"#how-to-use","title":"How to use?","text":"<ul> <li>The video tutorial \"Serivcenow Admin\" using this create questions. After watching the tutorial, you can customize the system to fit their specific needs and train it on their own data sources.</li> <li>Go to <code>Home</code> page and below section send your query.</li> <li>The question and answer system can be used to answer a wide range of questions related to the data sources it is trained on, such as product information, news articles, or historical events</li> <li>Users can ask natural language questions, such as \"What are the benefits of using Servicenow Admin?\" or \"How does the Serice Admin works?\"</li> <li>You may encounter limitations or challenges in using the OpenAI embeddings and Supabase vector database, such as the need for large amounts of training data or limitations in the accuracy of the model.</li> <li>However, with careful tuning and customization, the question and answer system can provide a highly effective way to access and analyze large volumes of data.</li> <li>Only ask question related this video.</li> </ul>"},{"location":"#upcoming-features","title":"Upcoming features","text":""},{"location":"#features","title":"Features","text":"<ul> <li>Upload any type of file, including text, PDFs, images, and more.</li> <li>Documents are processed and stored in a vector database for efficient querying.</li> <li>Users can ask questions using natural language queries.</li> <li>The application will search the database for documents that are relevant to the query.</li> <li>Documents are displayed to the user in a user-friendly format</li> </ul>"},{"location":"#future-improvements","title":"Future Improvements","text":"<ul> <li>Improve the accuracy of the natural language query processing.</li> <li>Add support for more types of documents.</li> <li>Allow users to tag documents for better organization.</li> <li>Add the ability to export documents as a single file.</li> </ul>"},{"location":"page2/","title":"Guide for Developers","text":""},{"location":"page2/#setup","title":"Setup","text":""},{"location":"page2/#overview","title":"Overview","text":"<p>This project involves converting YouTube videos to audio and then converting the audio to text. The text is then embedded using OpenAI's Embedding API and stored in a Supabase Vector database. The user can then query the database with a question and receive an answer from the database using Supabase's query functionality. Additionally, a Conversational Retrieval QA Chain is implemented using ChatGPT to provide more in-depth answers based on the user's previous questions. Here's a breakdown of the steps involved:</p> <ol> <li>Convert YouTube video to audio using ytdl-core library</li> <li>Convert each audio chunk to text using OpenAI's Whisper-1 API</li> <li>Store the text in a MongoDb database</li> <li>Split the text into smaller chunks using LangChain RecursiveSplitter</li> <li>Embed the text using OpenAI's Embedding API and store in a Supabase Vector database</li> <li>Query the Supabase Vector database using user input</li> <li>Use ChatGPT's Conversational Retrieval QA Chain to provide more in-depth answers based on the user's previous questions.</li> </ol> <p>By following these steps, users can obtain more detailed answers to their queries and have a richer experience with the system.</p>"},{"location":"page2/#prerequisites","title":"Prerequisites","text":"<p>Before you can set up a question and answer system using OpenAI embeddings and Supabase vector database, you will need to have the following prerequisites:</p> <ol> <li> <p>MERN Stack: You will need to have a working knowledge of the MERN (MongoDB, Express, React, Node.js) stack, as this is the technology stack used in the tutorial. This includes experience with React.js for building user interfaces, Node.js for server-side development, and MongoDB for database management.</p> </li> <li> <p>OpenAI API Key and Language Model: You will need an OpenAI API key and access to one of their language models, such as GPT-3. These APIs allow you to generate embeddings for text data, which are then stored and queried using Supabase vector database.</p> </li> <li> <p>Supabase Vector Database: You will need access to a Supabase vector database instance, which allows you to store and query high-dimensional vector data. Supabase provides an easy-to-use interface for managing your database and integrates seamlessly with the MERN stack.</p> </li> </ol> <p>In order to follow along with the tutorial and build your own question and answer system, you will need to have a working knowledge of these technologies and have access to the necessary resources. If you are unfamiliar with any of these technologies, we recommend that you spend some time learning them before attempting to build the question and answer system. Additionally, make sure that you have set up the necessary accounts and credentials before starting the tutorial.</p>"},{"location":"page2/#installtion","title":"Installtion","text":"<p>First create <code>server</code> folder and initialize with typescript and Install the below dependencies</p> <pre><code>npm i  langchain Express mongoose  @supabase/supabase-js   axios  form-data\n</code></pre> <p>Dev dependencies</p> <pre><code>npm i --save-dev nodemon @types/express @types/dotenv typescript ts-node\n</code></pre> <p>Make sure to setup typescript in your node js app. Use this article to config here or copy from gihub repo.</p>"},{"location":"page2/#convert-youtube-to-audio","title":"Convert Youtube to Audio","text":"<p>This project provides a function to convert a YouTube video to an audio file using ytdl-core and fs libraries.</p>"},{"location":"page2/#code-explanation","title":"Code Explanation","text":"<ul> <li>The ytdl function from ytdl-core library is used to fetch the YouTube video as a readable stream.</li> <li>A filter is applied to the formats available in the video to ensure that the chosen format has audio and is in the MP4 container.</li> <li>The pipe function is used to write the video stream to an output file as a writable stream using the <code>fs.createWriteStream</code> function.</li> <li>The on function is used to listen for the events 'finish' and 'error'. If the stream finishes without errors, the promise is resolved with the path of the output file. If an error occurs, the promise is rejected with the error object.</li> </ul> <pre><code>import ytdl from \"ytdl-core\";\nimport fs from \"fs\";\n\ninterface VideoFormat {\n  audioBitrate?: number;\n  container?: string;\n}\n\nexport const convertToAudio = (videoUrl: string): Promise&lt;string&gt; =&gt; {\n  const outputPath = \"audio.mp3\";\n  return new Promise((resolve, reject) =&gt; {\n    ytdl(videoUrl, {\n      filter: (format: VideoFormat): boolean =&gt;\n        !!format.audioBitrate &amp;&amp; format.container === \"mp4\",\n      quality: \"highestaudio\",\n    })\n      .on(\"error\", (err: Error) =&gt; {\n        reject(err);\n      })\n      .pipe(fs.createWriteStream(outputPath))\n      .on(\"finish\", () =&gt; {\n        resolve(outputPath);\n      })\n      .on(\"error\", (err: Error) =&gt; {\n        reject(err);\n      });\n  });\n};\n</code></pre>"},{"location":"page2/#audio-to-text-converter","title":"Audio to Text Converter","text":"<p>This project provides a function to convert an audio file to text using OpenAI's Whisper-1 API.</p>"},{"location":"page2/#code-explanation_1","title":"Code Explanation","text":"<ul> <li>The function uses the path and fs libraries to read the audio file from the file system and create a new <code>FormData</code> object.</li> <li>The FormData object is populated with the model and file fields. The model field specifies the OpenAI - API model to use for transcription, while the file field contains the audio file as a stream.</li> <li>An axios request is made to the OpenAI API with the FormData object as the request body and the Authorization header set with the <code>OPENAI_KEY</code> environment variable.</li> <li>The function returns a promise that resolves with the transcribed text from the OpenAI API.</li> </ul> <pre><code>import fs from \"fs\";\n\ninterface Translation {\n  text: String;\n}\n\nimport { Configuration, OpenAIApi } from \"openai\";\nimport { AxiosResponse } from \"axios\";\nexport const convertText = async (): Promise&lt;string&gt; =&gt; {\n  const configuration = new Configuration({\n    apiKey: process.env.OPENAI_KEY,\n  });\n  const openai = new OpenAIApi(configuration);\n  const resp:any = await openai.createTranslation(\n    fs.createReadStream(\"../server/assets/audio.mp3\"),\n    \"whisper-1\"\n  );\n  return resp.data.text;\n};\n\n};\n</code></pre>"},{"location":"page2/#store-on-mongodb","title":"Store on MongoDB","text":""},{"location":"page2/#code-explanation_2","title":"Code Explanation","text":"<ul> <li> <p>The code imports the <code>videoModel</code> object from the videoSchema module, which is defined in the models directory. This object is a Mongoose model that represents the schema for the videos collection in MongoDB.</p> </li> <li> <p>The storeMongoDb function takes a single argument, text, which is a string representing the transcript of a video that has been converted from audio to text using the OpenAI API.</p> </li> <li>Within the function, a new instance of the <code>videoModel</code> is created and passed an object with a single property, transcript, which is set to the text argument.</li> <li>The <code>save()</code> method is called on the new instance, which saves the object to the videos collection in the MongoDB database.</li> <li>If an error occurs during the saving process, the function returns a string indicating that the data was not stored successfully. Otherwise, it returns a string indicating that the data was stored successfully.</li> </ul> <pre><code>import { videoModel } from \"../../models/videoSchema\";\n\nexport const storeMongoDb = (text: String) =&gt; {\n  try {\n    const newVideo = new videoModel({\n      transcript: text,\n    });\n    newVideo.save();\n    return \"data stored succefully\";\n  } catch (error) {\n    return \"data stored succefully\";\n  }\n};\n</code></pre> <p>This is the Schema file :</p> <pre><code>import mongoose, { Model, Document } from \"mongoose\";\n\ninterface Text extends Document {\n  transcript: string;\n}\n\nconst videoSchema = new mongoose.Schema({\n  transcript: { type: String, required: true },\n});\n\nexport const videoModel: Model&lt;Text&gt; = mongoose.model&lt;Text&gt;(\"data\", videoSchema);\n</code></pre>"},{"location":"page2/#split-the-text","title":"Split the Text","text":""},{"location":"page2/#code-explanation_3","title":"Code Explanation","text":"<ul> <li>The code imports the RecursiveCharacterTextSplitter and Document classes from the langchain package, and the Text interface and videoModel from a local videoSchema module.</li> <li>The textSpilt function is defined with an async keyword, indicating that it will contain asynchronous operations.</li> <li>The function retrieves an array of Text documents from the MongoDB database using the find() method on the videoModel instance. The resulting array is assigned to a variable named result.</li> <li>A RecursiveCharacterTextSplitter instance is created with a configuration object specifying the size and overlap of text chunks.</li> <li>The createDocuments() method of the splitter instance is called with an array containing the transcript property of the first element in the result array. This converts the transcript into an array of Document objects.</li> <li>The resulting array of Document objects is returned from the function as a promise.</li> <li>Overall, the code retrieves text data from a MongoDB database, splits it into smaller chunks using a text splitter, and returns the resulting chunks as Document objects.</li> </ul> <pre><code>import { RecursiveCharacterTextSplitter } from \"langchain/text_splitter\";\nimport { Document } from \"langchain/document\";\nimport { Text, videoModel } from \"models/videoSchema\";\nexport const textSpilt = async (): Promise&lt;Document[]&gt; =&gt; {\n  const result: Text[] = await videoModel.find();\n  const splitter = new RecursiveCharacterTextSplitter({\n    chunkSize: 1000,\n    chunkOverlap: 0,\n  });\n\n  const docOutput = await splitter.createDocuments([result[0].transcript]);\n  return docOutput;\n};\n</code></pre>"},{"location":"page2/#embeddings-store-on-supabase","title":"Embeddings &amp; Store on Supabase","text":""},{"location":"page2/#code-explanation_4","title":"Code Explanation","text":"<ul> <li>The function first extracts the text content from each document in the input array.</li> <li>To get OpenAi Api here &amp;&amp; supabaseurl and supabase secretkey here</li> <li>It then creates a vector store using the SupabaseVectorStore class from the langchain/vectorstores/supabase module. This vector store will be used to perform similarity searches on the documents based on their embeddings.</li> <li>The function initializes the SupabaseVectorStore with the input documents, the OpenAI embeddings object, and a Supabase client and table name to store the vector representations of the documents.</li> <li>Finally, the function performs a similarity search on the vector store using the query \"She asked William to help her\". The result of the similarity search is logged to the console and returned from the function.</li> </ul> <pre><code>import { createClient, SupabaseClient } from \"@supabase/supabase-js\";\nimport {\n  SupabaseVectorStore,\n  SupabaseLibArgs,\n} from \"langchain/vectorstores/supabase\";\nimport { OpenAIEmbeddings } from \"langchain/embeddings/openai\";\nimport dotenv from \"dotenv\";\n\nconst OPENAI_KEY = process.env.OPENAI_KEY;\nconst supabaseUrl = process.env.supabaseUrl;\nconst supabaseKey = process.env.supabaseKey;\nexport const store = async (docs: any) =&gt; {\n  // const texts = [\"Hello world\", \"Bye bye\", \"What's this?\"];\n  const texts = docs.map((i: any) =&gt; i.pageContent);\n  const vectorStore = await SupabaseVectorStore.fromDocuments(\n    docs,\n    new OpenAIEmbeddings({ openAIApiKey: OPENAI_KEY }),\n    {\n      client: createClient(supabaseUrl, supabaseKey) as SupabaseClient,\n      tableName: \"documents\",\n    } as SupabaseLibArgs // assert that the object conforms to SupabaseLibArgs type\n  );\n\n  const resultOne = await vectorStore.similaritySearch(\n    \"She asked William to help her\"\n  );\n\n  console.log(resultOne);\n  return resultOne;\n};\n</code></pre>"},{"location":"page2/#query","title":"Query","text":""},{"location":"page2/#code-explanation_5","title":"Code Explanation","text":"<ul> <li>It imports several modules: OpenAI from <code>langchain/llms/openai</code>, <code>ConversationalRetrievalQAChain</code> from <code>langchain/chains</code>, <code>SupabaseVectorStore</code> from <code>langchain/vectorstores/supabase</code>, and createClient from <code>@supabase/supabase-js</code>.</li> <li>It sets the values of <code>OPENAI_KEY</code>, <code>supabaseUrl</code>, and <code>supabaseKey</code> by ( click here to get supabaseurl and Supabasekey )accessing the environment variables. To get Api key click</li> <li>It creates a new instance of the OpenAI class using the <code>gpt-3.5-turbo</code> model and the <code>OPENAI_KEY</code>.</li> <li>It creates a <code>SupabaseVectorStore</code> object from an existing index named match_documents in a Supabase database table named documents using the createClient function and supabaseUrl and supabaseKey.</li> <li>It creates a ConversationalRetrievalQAChain object from the OpenAI instance and the <code>SupabaseVectorStore</code> object as the retriever.</li> <li>It calls the call method of the <code>ConversationalRetrievalQAChain</code> object, passing an object that contains the question and an empty chat_history.</li> <li>It returns the result of the call method.</li> </ul> <pre><code>import { OpenAI } from \"langchain/llms/openai\";\nimport { ConversationalRetrievalQAChain } from \"langchain/chains\";\nimport { SupabaseVectorStore } from \"langchain/vectorstores/supabase\";\nimport { OpenAIEmbeddings } from \"langchain/embeddings/openai\";\nimport { createClient } from \"@supabase/supabase-js\";\n\nconst OPENAI_KEY = process.env.OPENAI_KEY;\nconst supabaseUrl = process.env.supabaseUrl;\nconst supabaseKey = process.env.supabaseKey;\nconst client = createClient(supabaseUrl, supabaseKey);\n\nexport const query = async (query: any) =&gt; {\n  const model = new OpenAI({\n    modelName: \"gpt-3.5-turbo\",\n    openAIApiKey: OPENAI_KEY,\n  });\n\n  // console.log(client)\n  const vectorStore = await SupabaseVectorStore.fromExistingIndex(\n    new OpenAIEmbeddings({ openAIApiKey: OPENAI_KEY }),\n    {\n      client,\n      tableName: \"documents\",\n      queryName: \"match_documents\",\n    }\n  );\n\n  /* Create the chain */\n  const chain = ConversationalRetrievalQAChain.fromLLM(\n    model,\n    vectorStore.asRetriever()\n  );\n\n  const res = await chain.call({ question: query, chat_history: [] });\n\n  return res;\n};\n</code></pre>"}]}